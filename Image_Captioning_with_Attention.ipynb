{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Image Captioning Basic Model CNN + LSTM\n"
      ],
      "metadata": {
        "id": "rdYCtezo__B5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Flickr8k DataSet for Image Captioning"
      ],
      "metadata": {
        "id": "OvGRvCzr29Ds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "ppiT9OXYH4NS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3ScHhExH--L",
        "outputId": "c8f05f7b-6edf-4fb9-f55c-b19ca8212833"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "archive = zipfile.ZipFile('drive/MyDrive/archive.zip')\n",
        "\n",
        "for file in archive.namelist():\n",
        "    archive.extract(file, 'CaptionData')"
      ],
      "metadata": {
        "id": "60GnSJQkd-ol"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Iwpk8IEIefxe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "caption_df = pd.read_csv('CaptionData/captions.txt')\n",
        "caption_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "1gUbsKNaelZW",
        "outputId": "67620370-7cc0-46cb-8cc3-5e1b7e4649a0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       image  \\\n",
              "0  1000268201_693b08cb0e.jpg   \n",
              "1  1000268201_693b08cb0e.jpg   \n",
              "2  1000268201_693b08cb0e.jpg   \n",
              "3  1000268201_693b08cb0e.jpg   \n",
              "4  1000268201_693b08cb0e.jpg   \n",
              "\n",
              "                                             caption  \n",
              "0  A child in a pink dress is climbing up a set o...  \n",
              "1              A girl going into a wooden building .  \n",
              "2   A little girl climbing into a wooden playhouse .  \n",
              "3  A little girl climbing the stairs to her playh...  \n",
              "4  A little girl in a pink dress going into a woo...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9de8fb92-f8f2-4326-b645-2f346e5f37d4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>caption</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>A child in a pink dress is climbing up a set o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>A girl going into a wooden building .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>A little girl climbing into a wooden playhouse .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>A little girl climbing the stairs to her playh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>A little girl in a pink dress going into a woo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9de8fb92-f8f2-4326-b645-2f346e5f37d4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9de8fb92-f8f2-4326-b645-2f346e5f37d4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9de8fb92-f8f2-4326-b645-2f346e5f37d4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7a57bb66-8b8e-450c-b4ec-1a88c57d5c52\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7a57bb66-8b8e-450c-b4ec-1a88c57d5c52')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7a57bb66-8b8e-450c-b4ec-1a88c57d5c52 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "caption_df",
              "summary": "{\n  \"name\": \"caption_df\",\n  \"rows\": 40455,\n  \"fields\": [\n    {\n      \"column\": \"image\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8091,\n        \"samples\": [\n          \"3139895886_5a6d495b13.jpg\",\n          \"3133825703_359a0c414d.jpg\",\n          \"244910177_7c4ec3f65b.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"caption\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 40201,\n        \"samples\": [\n          \"A girl plays T-ball .\",\n          \"A woman in riding attire rides a jumping horse .\",\n          \"A brown dog wearing a pink shirt is followed by a brown dog wearing a yellow shirt .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean the captions\n",
        "\n",
        "import string\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def clean_caption(caption):\n",
        "    \"\"\"\n",
        "    Clean individual caption text.\n",
        "    \"\"\"\n",
        "    # Convert to lowercase\n",
        "    caption = caption.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    caption = caption.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Remove numbers\n",
        "    caption = re.sub(r'\\d+', '', caption)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    caption = caption.strip()\n",
        "    caption = re.sub(r'\\s+', ' ', caption)\n",
        "\n",
        "    # Tokenize the caption\n",
        "    tokens = word_tokenize(caption)\n",
        "\n",
        "    # # Remove short and long words (optional)\n",
        "    # tokens = [word for word in tokens if len(word) > 1 and len(word) < 15]\n",
        "\n",
        "    # # Remove stopwords (optional)\n",
        "    # stop_words = set(stopwords.words('english'))\n",
        "    # tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Rejoin tokens into a single string\n",
        "    cleaned_caption = ' '.join(tokens)\n",
        "\n",
        "    return cleaned_caption\n",
        "\n",
        "def clean_captions(captions):\n",
        "    \"\"\"\n",
        "    Clean a list of captions.\n",
        "    \"\"\"\n",
        "    cleaned_captions = [clean_caption(caption) for caption in captions]\n",
        "    return cleaned_captions\n",
        "\n",
        "# Example usage\n",
        "captions = [\n",
        "    \"A dog is playing with a ball.\",\n",
        "    \"Children are playing in the playground!\",\n",
        "    \"An airplane is flying in the sky, #Amazing!\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"123 cats are sleeping on the couch.\"\n",
        "]\n",
        "\n",
        "cleaned_captions = clean_captions(captions)\n",
        "print(cleaned_captions)\n"
      ],
      "metadata": {
        "id": "goJtHwA-mRXM",
        "outputId": "ac52d053-6b32-4287-e663-a582fb531d8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a dog is playing with a ball', 'children are playing in the playground', 'an airplane is flying in the sky amazing', 'the quick brown fox jumps over the lazy dog', 'cats are sleeping on the couch']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "caption_df['caption'] = caption_df['caption'].apply(lambda x: clean_caption(x))"
      ],
      "metadata": {
        "id": "hX5qxoBWmpoi"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "caption_df.head()"
      ],
      "metadata": {
        "id": "9FlRZmFYm282",
        "outputId": "0fecda27-153b-4d7e-b922-2233206b6033",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       image  \\\n",
              "0  1000268201_693b08cb0e.jpg   \n",
              "1  1000268201_693b08cb0e.jpg   \n",
              "2  1000268201_693b08cb0e.jpg   \n",
              "3  1000268201_693b08cb0e.jpg   \n",
              "4  1000268201_693b08cb0e.jpg   \n",
              "\n",
              "                                             caption  \n",
              "0  a child in a pink dress is climbing up a set o...  \n",
              "1                a girl going into a wooden building  \n",
              "2     a little girl climbing into a wooden playhouse  \n",
              "3  a little girl climbing the stairs to her playh...  \n",
              "4  a little girl in a pink dress going into a woo...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-14c314da-9faf-4b69-b6de-fcefbf904de7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>caption</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>a child in a pink dress is climbing up a set o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>a girl going into a wooden building</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>a little girl climbing into a wooden playhouse</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>a little girl climbing the stairs to her playh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>a little girl in a pink dress going into a woo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-14c314da-9faf-4b69-b6de-fcefbf904de7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-14c314da-9faf-4b69-b6de-fcefbf904de7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-14c314da-9faf-4b69-b6de-fcefbf904de7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-987ef68b-d4f0-48a0-87e8-fd2f22a79848\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-987ef68b-d4f0-48a0-87e8-fd2f22a79848')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-987ef68b-d4f0-48a0-87e8-fd2f22a79848 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "caption_df",
              "summary": "{\n  \"name\": \"caption_df\",\n  \"rows\": 40455,\n  \"fields\": [\n    {\n      \"column\": \"image\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8091,\n        \"samples\": [\n          \"3139895886_5a6d495b13.jpg\",\n          \"3133825703_359a0c414d.jpg\",\n          \"244910177_7c4ec3f65b.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"caption\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 40123,\n        \"samples\": [\n          \"a man with no shirt is climbing a rock ledge\",\n          \"a girl and boy with sunglasses and a red car behind them\",\n          \"blue and silver car going around curve being watched by people standing in grass\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_ids = list(caption_df['image'].values)\n",
        "len(image_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EDiQI9ogRuW",
        "outputId": "5fbf5969-efa1-4fbc-ce48-7ed4c8acd463"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40455"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_image_ids = caption_df['image'].unique()"
      ],
      "metadata": {
        "id": "0kkWzvrggqlp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess the Data"
      ],
      "metadata": {
        "id": "A2nTTDzN20wO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Images\n",
        "import os\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "IMAGE_SHAPE = (299, 299, 3)\n",
        "\n",
        "def preprocess_image(img_path):\n",
        "  img = image.load_img(img_path)\n",
        "  img = image.img_to_array(img)\n",
        "  img = img/255.0\n",
        "  img.resize(IMAGE_SHAPE)\n",
        "  img = preprocess_input(img)\n",
        "  img = np.expand_dims(img, axis=0)\n",
        "  return img\n"
      ],
      "metadata": {
        "id": "w4vZ9NGWeyvT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = preprocess_image('/content/CaptionData/Images/1007129816_e794419615.jpg')\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V245KwNdt6uK",
        "outputId": "34b7f706-7405-4402-c1e3-c3707829783b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 299, 299, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For ease of computation here dealing with less number of images\n",
        "image_ids = image_ids[:5000]"
      ],
      "metadata": {
        "id": "q4_0PpM9_SYK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def encode_images(image_dir, image_ids):\n",
        "  model = InceptionV3(weights='imagenet')\n",
        "  model = Model(model.input, model.layers[-2].output, name='feature_extractor')\n",
        "  image_features = {}\n",
        "\n",
        "  for image_id in image_ids:\n",
        "    if image_id not in image_features:\n",
        "      try:\n",
        "        img = preprocess_image(os.path.join(image_dir, image_id))\n",
        "        feature = model.predict(img, verbose=0)\n",
        "        image_features[image_id] = feature\n",
        "      except Exception as e:\n",
        "        print(f\"Error processing image {image_id}: {e}\")\n",
        "  return image_features\n",
        "\n",
        "image_dir = '/content/CaptionData/Images'\n",
        "image_features = encode_images(image_dir, image_ids)\n",
        "\n",
        "len(image_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4bCJXJJg5dr",
        "outputId": "63053d15-b82c-4e16-c63d-1363334e87f7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Total number of unique images extracted: {len(image_features)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_6L7LIQt_oZ",
        "outputId": "a5b2d06a-c57b-435a-dd36-f8bf461f0a21"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of unique images extracted: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('image_features.pkl', 'wb') as f:\n",
        "    pickle.dump(image_features, f)"
      ],
      "metadata": {
        "id": "eelWZ9JF5MH5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Load the Captions\n",
        "\n",
        "captions_map = dict()\n",
        "all_captions = [] # populating to find out the vocabulary size\n",
        "\n",
        "\n",
        "def load_captions(captions_file_path):\n",
        "  with open(captions_file_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "    for line in lines:\n",
        "      tokens = line.split(',')\n",
        "      image_id, caption = tokens[0], ' '.join(tokens[1:]).lower()\n",
        "      if image_id in image_ids:\n",
        "        caption = clean_caption(caption)\n",
        "        caption = '<startseq> ' + caption + ' <endseq>'\n",
        "        if image_id not in captions_map:\n",
        "          captions_map[image_id] = []\n",
        "\n",
        "        captions_map[image_id].append(caption)\n",
        "        all_captions.append(caption)\n",
        "\n",
        "load_captions('/content/CaptionData/captions.txt')"
      ],
      "metadata": {
        "id": "FFpa4UfAZXCo"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(captions_map)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcjLiPQtuXIk",
        "outputId": "d1753c1a-d58e-4a06-b9cb-fd63bcb55de6"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('captions_map.pkl', 'wb') as f:\n",
        "    pickle.dump(captions_map, f)"
      ],
      "metadata": {
        "id": "TLQvbc_8-hmR"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "m4dbFScf6_dV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_captions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Vocabulary size:\", vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLI_RmhSyQTy",
        "outputId": "29dc01a4-0b7c-48bc-d34e-15999f8da610"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 3224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert captions to sequences\n",
        "sequences = tokenizer.texts_to_sequences(all_captions)\n",
        "\n",
        "# Find Max Length of a sequence\n",
        "max_length = max(len(seq) for seq in sequences)\n",
        "print(\"Max length of sequences:\", max_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWrvTzF1gjnw",
        "outputId": "1bac3e6c-f336-480d-a47d-9a50437ac5ee"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max length of sequences: 35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import groupby\n",
        "from collections import Counter\n",
        "\n",
        "sequence_lengths = [len(seq) for seq in sequences]\n",
        "sequence_length_counts = dict(Counter(sequence_lengths))\n",
        "sequence_length_counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDVrqHA380Fr",
        "outputId": "76eeea0b-308a-42e4-b1ad-0f6e6dc2bddd"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{19: 132,\n",
              " 9: 393,\n",
              " 10: 439,\n",
              " 11: 544,\n",
              " 14: 467,\n",
              " 17: 212,\n",
              " 20: 120,\n",
              " 21: 58,\n",
              " 22: 39,\n",
              " 15: 402,\n",
              " 16: 307,\n",
              " 13: 544,\n",
              " 18: 178,\n",
              " 12: 567,\n",
              " 7: 126,\n",
              " 8: 315,\n",
              " 25: 13,\n",
              " 4: 1,\n",
              " 24: 18,\n",
              " 26: 13,\n",
              " 6: 43,\n",
              " 28: 6,\n",
              " 23: 25,\n",
              " 29: 5,\n",
              " 31: 1,\n",
              " 27: 4,\n",
              " 5: 21,\n",
              " 35: 3,\n",
              " 30: 2,\n",
              " 32: 1,\n",
              " 33: 1}"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's say we take max caption token length is 20\n",
        "max_length = 20\n"
      ],
      "metadata": {
        "id": "cTYEKv_o-Mi2"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Image features and Captions Map if stored in pickle file\n",
        "import pickle\n",
        "\n",
        "with open('image_features.pkl', 'rb') as f:\n",
        "  image_features = pickle.load(f)\n",
        "\n",
        "with open('captions_map.pkl', 'rb') as f:\n",
        "  captions_map = pickle.load(f)"
      ],
      "metadata": {
        "id": "BCfsKI9H80rN"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(image_features), len(captions_map)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkKkwfKI9PGN",
        "outputId": "5cd2af43-3dcf-40fc-de63-8760b928f1fe"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 1000)"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Training Data\n",
        "\n",
        "Training Data Input:\n",
        "\n",
        "\n",
        "\n",
        "*   Image\n",
        "*   Partial Caption output text so far\n",
        "\n",
        "\n",
        "Training Data Output:\n",
        "\n",
        "\n",
        "*   Next Caption token\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8O9wXf-g8817"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_training_data(captions_map, image_ids, image_features, tokenizer, max_length):\n",
        "  X1, X2, y = [], [], []\n",
        "  for image_id in image_ids:\n",
        "    captions = captions_map[image_id]\n",
        "    for caption in captions:\n",
        "      seq = tokenizer.texts_to_sequences([caption])[0]\n",
        "      for i in range(1, len(seq)):\n",
        "        in_seq, out_seq = seq[:i], seq[i]\n",
        "        in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "        # X1.append(np.squeeze(image_features[image_id], axis=0))\n",
        "        X1.append(image_features[image_id])\n",
        "        X2.append(in_seq)\n",
        "        y.append(out_seq)\n",
        "  return np.array(X1), np.array(X2), np.array(y)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H9XgojPk-qX6"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_ids = list(image_features.keys())"
      ],
      "metadata": {
        "id": "cDumg-Gu_aio"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_image_ids = image_ids[:800]\n",
        "val_image_ids = image_ids[800:900]\n",
        "test_image_ids = image_ids[900:]"
      ],
      "metadata": {
        "id": "5DonKOcjcBrS"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6tgSUAbQcQF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_X1, train_data_X2, train_data_y = generate_training_data(captions_map, train_image_ids, image_features, tokenizer, max_length)"
      ],
      "metadata": {
        "id": "giH1yTTpZAl7"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_X1.shape, train_data_X2.shape, train_data_y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46NxEFki_259",
        "outputId": "594f3d09-ba1b-419f-fa7b-100cf7c0a3ee"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((47845, 1, 2048), (47845, 20), (47845, 3224))"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_data_X1, val_data_X2, val_data_y = generate_training_data(captions_map, val_image_ids, image_features, tokenizer, max_length)"
      ],
      "metadata": {
        "id": "ptX30SuKf8Ip"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the prepared Data\n",
        "np.save('train_data_X1.npy', train_data_X1)\n",
        "np.save('train_data_X2.npy', train_data_X2)\n",
        "np.save('train_data_y.npy', train_data_y)\n",
        "np.save('val_data_X1.npy', val_data_X1)\n",
        "np.save('val_data_X2.npy', val_data_X2)\n",
        "np.save('val_data_y.npy', val_data_y)"
      ],
      "metadata": {
        "id": "X43z7nljqmze"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_X1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWlaMkovADvl",
        "outputId": "a7cee505-7f97-4500-f4c3-d33cf9a1aa58"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(48082, 1, 2048)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load train and test data if saved earlier\n",
        "\n",
        "train_data_X1 = np.load('train_data_X1.npy')\n",
        "train_data_X2 = np.load('train_data_X2.npy')\n",
        "train_data_y = np.load('train_data_y.npy')\n",
        "\n",
        "val_data_X1 = np.load('val_data_X1.npy')\n",
        "val_data_X2 = np.load('val_data_X2.npy')\n",
        "val_data_y = np.load('val_data_y.npy')"
      ],
      "metadata": {
        "id": "iu2mCe9r9eM3"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3o0CWTRaBZwl",
        "outputId": "76529bbe-d8ab-4118-e5ac-7e195588a27d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_9\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " text_input (InputLayer)     [(None, 20)]                 0         []                            \n",
            "                                                                                                  \n",
            " image_input (InputLayer)    [(None, 1, 2048)]            0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 20, 256)              825344    ['text_input[0][0]']          \n",
            "                                                                                                  \n",
            " dense1 (Dense)              (None, 1, 256)               524544    ['image_input[0][0]']         \n",
            "                                                                                                  \n",
            " multi_head_attention (Mult  ((None, 20, 256),            2103552   ['embedding[0][0]',           \n",
            " iHeadAttention)              (None, 8, 20, 1))                      'dense1[0][0]']              \n",
            "                                                                                                  \n",
            " decoder_input (Concatenate  (None, 20, 512)              0         ['multi_head_attention[0][0]',\n",
            " )                                                                   'embedding[0][0]']           \n",
            "                                                                                                  \n",
            " gru_10 (GRU)                (None, 20, 256)              591360    ['decoder_input[0][0]']       \n",
            "                                                                                                  \n",
            " gru_11 (GRU)                (None, 256)                  394752    ['gru_10[0][0]']              \n",
            "                                                                                                  \n",
            " dense_33 (Dense)            (None, 3224)                 828568    ['gru_11[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5268120 (20.10 MB)\n",
            "Trainable params: 5268120 (20.10 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, MultiHeadAttention, Concatenate, Lambda, GRU, Flatten\n",
        "\n",
        "# Feature extractor (encoder) model\n",
        "image_input = Input(shape=(1, 2048,), name='image_input')\n",
        "# image_input = Input(shape=(8 * 8, 2048))\n",
        "image_features = Dense(256, activation='relu', name='dense1')(image_input)\n",
        "# image_features = Flatten()(fe1)\n",
        "\n",
        "# Sequence processor (decoder) model\n",
        "text_input = Input(shape=(max_length,), name='text_input')\n",
        "text_embedding = Embedding(vocab_size, 256, mask_zero=True, name='embedding')(text_input)\n",
        "# text_embedding = Flatten()(text_embedding)\n",
        "\n",
        "attention_output, attention_weights = MultiHeadAttention(num_heads=8,\n",
        "                                                         key_dim=256,\n",
        "                                                         name='multi_head_attention')(query=text_embedding,\n",
        "                                                                      value=image_features,\n",
        "                                                                      return_attention_scores=True)\n",
        "# context_vector = attention_weights * image_features\n",
        "# context_vector = Lambda(lambda x: tf.reduce_sum(x, axis=1), name='context_vector')(context_vector)\n",
        "\n",
        "\n",
        "\n",
        "decoder_input = Concatenate(axis=-1, name='decoder_input')([attention_output, text_embedding])\n",
        "\n",
        "\n",
        "# Decoder\n",
        "\n",
        "decoder_lstm = GRU(256, return_sequences=True)(decoder_input)\n",
        "decoder_lstm = GRU(256)(decoder_lstm)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder_lstm)\n",
        "\n",
        "# Combined model\n",
        "model = Model(inputs=[image_input, text_input], outputs=outputs)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)"
      ],
      "metadata": {
        "id": "-m4eaR1jKDhw"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    'content/checkpoint',\n",
        "    monitor=\"val_loss\",\n",
        "    verbose=0)"
      ],
      "metadata": {
        "id": "2s_IsR0pSSYI"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41Pi5UrPBsjf",
        "outputId": "f03f724f-9b91-470d-cd84-5bf1e54c61e1"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1503/1503 - 142s - loss: 5.2052 - accuracy: 0.1741 - val_loss: 5.3199 - val_accuracy: 0.1895 - 142s/epoch - 95ms/step\n",
            "Epoch 2/20\n",
            "1503/1503 - 124s - loss: 4.9473 - accuracy: 0.1869 - val_loss: 5.4138 - val_accuracy: 0.1804 - 124s/epoch - 83ms/step\n",
            "Epoch 3/20\n",
            "1503/1503 - 126s - loss: 4.8994 - accuracy: 0.1880 - val_loss: 5.4043 - val_accuracy: 0.1895 - 126s/epoch - 84ms/step\n",
            "Epoch 4/20\n",
            "1503/1503 - 125s - loss: 4.8813 - accuracy: 0.1894 - val_loss: 5.4488 - val_accuracy: 0.1804 - 125s/epoch - 83ms/step\n",
            "Epoch 5/20\n",
            "1503/1503 - 127s - loss: 4.8630 - accuracy: 0.1899 - val_loss: 5.4887 - val_accuracy: 0.1777 - 127s/epoch - 84ms/step\n",
            "Epoch 6/20\n",
            "1503/1503 - 125s - loss: 4.8413 - accuracy: 0.1889 - val_loss: 5.5240 - val_accuracy: 0.1838 - 125s/epoch - 83ms/step\n",
            "Epoch 7/20\n",
            "1503/1503 - 124s - loss: 4.8320 - accuracy: 0.1897 - val_loss: 5.5615 - val_accuracy: 0.1905 - 124s/epoch - 83ms/step\n",
            "Epoch 8/20\n",
            "1503/1503 - 126s - loss: 4.8159 - accuracy: 0.1896 - val_loss: 5.5899 - val_accuracy: 0.1839 - 126s/epoch - 84ms/step\n",
            "Epoch 9/20\n",
            "1503/1503 - 125s - loss: 4.8126 - accuracy: 0.1896 - val_loss: 5.5931 - val_accuracy: 0.1895 - 125s/epoch - 83ms/step\n",
            "Epoch 10/20\n",
            "1503/1503 - 124s - loss: 4.8004 - accuracy: 0.1896 - val_loss: 5.6427 - val_accuracy: 0.1804 - 124s/epoch - 83ms/step\n",
            "Epoch 11/20\n",
            "1503/1503 - 123s - loss: 4.7965 - accuracy: 0.1890 - val_loss: 5.6421 - val_accuracy: 0.1877 - 123s/epoch - 82ms/step\n",
            "Epoch 12/20\n",
            "1503/1503 - 125s - loss: 4.7869 - accuracy: 0.1891 - val_loss: 5.6546 - val_accuracy: 0.1866 - 125s/epoch - 83ms/step\n",
            "Epoch 13/20\n",
            "1503/1503 - 124s - loss: 4.7736 - accuracy: 0.1895 - val_loss: 5.7092 - val_accuracy: 0.1895 - 124s/epoch - 83ms/step\n",
            "Epoch 14/20\n",
            "1503/1503 - 124s - loss: 4.7715 - accuracy: 0.1888 - val_loss: 5.7249 - val_accuracy: 0.1905 - 124s/epoch - 83ms/step\n",
            "Epoch 15/20\n",
            "1503/1503 - 124s - loss: 4.7514 - accuracy: 0.1905 - val_loss: 5.7341 - val_accuracy: 0.1905 - 124s/epoch - 83ms/step\n",
            "Epoch 16/20\n",
            "1503/1503 - 125s - loss: 4.7496 - accuracy: 0.1905 - val_loss: 5.7302 - val_accuracy: 0.1848 - 125s/epoch - 83ms/step\n",
            "Epoch 17/20\n",
            "1503/1503 - 123s - loss: 4.7369 - accuracy: 0.1906 - val_loss: 5.7733 - val_accuracy: 0.1905 - 123s/epoch - 82ms/step\n",
            "Epoch 18/20\n",
            "1503/1503 - 125s - loss: 4.7292 - accuracy: 0.1894 - val_loss: 5.7947 - val_accuracy: 0.1838 - 125s/epoch - 83ms/step\n",
            "Epoch 19/20\n",
            "1503/1503 - 124s - loss: 4.7309 - accuracy: 0.1898 - val_loss: 5.7721 - val_accuracy: 0.1905 - 124s/epoch - 82ms/step\n",
            "Epoch 20/20\n",
            "1503/1503 - 124s - loss: 4.7255 - accuracy: 0.1904 - val_loss: 5.7482 - val_accuracy: 0.1895 - 124s/epoch - 83ms/step\n"
          ]
        }
      ],
      "source": [
        "history = model.fit([train_data_X1, train_data_X2], train_data_y,\n",
        "          validation_data=([val_data_X1, val_data_X2], val_data_y),\n",
        "          epochs=20,\n",
        "          verbose=2,\n",
        "          callbacks=[early_stopping, model_checkpoint]\n",
        "          )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history2 = model.fit([train_data_X1, train_data_X2], train_data_y,\n",
        "          # validation_data=([val_data_X1, val_data_X2], val_data_y),\n",
        "          epochs=1,\n",
        "          verbose=2\n",
        "          # callbacks=[early_stopping, model_checkpoint]\n",
        "          )"
      ],
      "metadata": {
        "id": "Z3P5Qnibp-Zn",
        "outputId": "73f27d68-a89d-45c8-b2e8-9433175e1791",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1496/1496 - 110s - loss: 4.7097 - accuracy: 0.1909 - 110s/epoch - 73ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('image_captioning_model_2.h5')"
      ],
      "metadata": {
        "id": "csFCw0UEM6ZF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a08b40b4-5cce-42ca-95aa-1e20b133f636"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "1jYM42KxiWOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_image(image_path):\n",
        "  img = preprocess_image(image_path)\n",
        "  inception_v3 = InceptionV3(weights='imagenet')\n",
        "  inception_v3 = Model(inception_v3.input, inception_v3.layers[-2].output, name='feature_extractor')\n",
        "  feature = inception_v3.predict(img, verbose=0)\n",
        "  return feature"
      ],
      "metadata": {
        "id": "vtoHvor6C3e6"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "t4uaSKeFDmZR"
      },
      "outputs": [],
      "source": [
        "def generate_caption(model, tokenizer, image, max_length):\n",
        "    in_text = '<startseq>'\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "\n",
        "    for i in range(max_length):\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        # print(sequence.shape, image.shape)\n",
        "        yhat = model.predict([image, sequence], verbose=0)\n",
        "        yhat = np.argmax(yhat)\n",
        "        word = tokenizer.index_word[yhat]\n",
        "        if word is None:\n",
        "            break\n",
        "        in_text += ' ' + word\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    return in_text\n",
        "\n",
        "# # Example usage\n",
        "# img_path = 'fish-8896355_1280.jpg'\n",
        "# image_feature = encode_image(img_path)\n",
        "# image_feature = np.expand_dims(image_feature, axis=0)\n",
        "# caption = generate_caption(model, tokenizer, image_feature, max_length)\n",
        "# print(\"Generated Caption:\", caption)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Images\n",
        "\n",
        "test_image_features = [image_features[image_id] for image_id in test_image_ids]\n",
        "test_image_features = np.array(test_image_features)\n",
        "test_image_features.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbQEjEQVFJwh",
        "outputId": "fef0d7b8-4900-455e-985e-82ce976ec830"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 1, 2048)"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_captions = [captions_map[image_id] for image_id in test_image_ids]\n",
        "len(test_captions), test_captions[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESRrQ28SGJlf",
        "outputId": "c3a528ab-1afc-4e20-e524-03151676dda6"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,\n",
              " ['<startseq> a group of horses and people in front of a snowy mountain .\\n <endseq>',\n",
              "  '<startseq> the riders and horses are taking a break and resting on the mountain trail .\\n <endseq>',\n",
              "  '<startseq> three men are standing around pack horses in front of a red tent up in the mountains .\\n <endseq>',\n",
              "  '<startseq> three riders stand around their horses in the mountains .\\n <endseq>',\n",
              "  '<startseq> two men and some horses on a snowy mountain .\\n <endseq>'])"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_features[test_image_ids[0]].shape"
      ],
      "metadata": {
        "id": "Lc00cT0Jj8lT",
        "outputId": "2248d2b8-f6f7-46aa-f210-90a9b78e7df9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 2048)"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, image_id in enumerate(test_image_ids[:10]):\n",
        "  print(generate_caption(model, tokenizer, image_features[image_id], max_length))\n",
        "  print(captions_map[image_id])\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2XRhK9vHycw",
        "outputId": "f996f79e-74d7-4009-a631-54fe55a45c41"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<startseq> a man in a a a a a endseq\n",
            "['<startseq> a group of horses and people in front of a snowy mountain .\\n <endseq>', '<startseq> the riders and horses are taking a break and resting on the mountain trail .\\n <endseq>', '<startseq> three men are standing around pack horses in front of a red tent up in the mountains .\\n <endseq>', '<startseq> three riders stand around their horses in the mountains .\\n <endseq>', '<startseq> two men and some horses on a snowy mountain .\\n <endseq>']\n",
            "\n",
            "<startseq> a man in a a a a a endseq\n",
            "['<startseq> a black and brown dog is biting on a stick in the forest .\\n <endseq>', '<startseq> a brown dog chewing on a large piece of wood .\\n <endseq>', '<startseq> a brown dog is chewing on a stick .\\n <endseq>', '<startseq> a dark brown dog is chewing on a stick .\\n <endseq>', '<startseq> the brown dog is playing with a stick .\\n <endseq>']\n",
            "\n",
            "<startseq> a man in a a a a a endseq\n",
            "['<startseq> a black dog carrying a colorful ball swims .\\n <endseq>', '<startseq> a black dog is retrieving a ball in water .\\n <endseq>', '<startseq> a black dog is swimming with a ball in his mouth .\\n <endseq>', '<startseq> a black dog swims in water with a colorful ball in his mouth .\\n <endseq>', '<startseq> black dog paddles through the water with a bright ball in its mouth .\\n <endseq>']\n",
            "\n",
            "<startseq> a man in a a a a a endseq\n",
            "['<startseq> a brown dog is jumping up at another dog in front of the man in jeans .\\n <endseq>', '<startseq> a tan dog is grappling another tan dog on gravel while someone stands narby .\\n <endseq>', '<startseq> two dogs engaged in physical contact with a man in the background .\\n <endseq>', '<startseq> two dogs play fighting on sand .\\n <endseq>', '<startseq> two dogs wrestle or hug .\\n <endseq>']\n",
            "\n",
            "<startseq> a man in a a a a a endseq\n",
            "['<startseq> a hiker posing for a photo in an arid mountain landscape .\\n <endseq>', '<startseq> a man in a green hat poses in the mountains\\n <endseq>', '<startseq> a man in a green hat standing on a slope in front of a mountain .\\n <endseq>', '<startseq> \"a man in camouflage hiking gear   wearing a green pouch is standing in the foothills .\"\\n <endseq>', '<startseq> a person in hiking gear is standing on a hill with a mountain in the background .\\n <endseq>']\n",
            "\n",
            "<startseq> a man in a a a a a endseq\n",
            "['<startseq> a boy gets flipped into the river by another boy .\\n <endseq>', '<startseq> a group of children play in the water under a bridge .\\n <endseq>', '<startseq> an airborne kid is watched by his launcher and others .\\n <endseq>', '<startseq> boys jump off a bridge into the water .\\n <endseq>', '<startseq> man in white shirt flipping young boy in the water with four other boys surrounding them .\\n <endseq>']\n",
            "\n",
            "<startseq> a man in a a a a a endseq\n",
            "[\"<startseq> a little girl in a blue and pink leotard is walking along a beam whilst a person 's hand is stretched out to support her .\\n <endseq>\", '<startseq> a little girl in a blue and pink outfit is walking on a balance beam with the aid of woman standing to her left .\\n <endseq>', '<startseq> a little girl walking down a balance beam with an adult ready to assist .\\n <endseq>', '<startseq> a young girl is standing on a balance beam .\\n <endseq>', '<startseq> a young girl practicing gymnastics on a balance beam .\\n <endseq>']\n",
            "\n",
            "<startseq> a man in a a a a a endseq\n",
            "['<startseq> a barefoot young boy glances at the camera while swinging in a park .\\n <endseq>', '<startseq> a boy swings high under blue sky .\\n <endseq>', '<startseq> a boy wearing brown swings in a playground against a blue sky .\\n <endseq>', '<startseq> a young sandy blonde boy is at a park swinging on the swings .\\n <endseq>', '<startseq> two children at the height of a swing .\\n <endseq>']\n",
            "\n",
            "<startseq> a man in a a a a a endseq\n",
            "['<startseq> the men are climbing .\\n <endseq>', '<startseq> two men are lying in red cots on the side of a mountain .\\n <endseq>', '<startseq> two men are sleeping on makeshift beds on the side of a cliff .\\n <endseq>', '<startseq> two men in cots hanging from a cliff\\n <endseq>', '<startseq> two mountain climbers are smiling on the mountainside .\\n <endseq>']\n",
            "\n",
            "<startseq> a man in a a a a a endseq\n",
            "['<startseq> a little girl in pajamas is jumping on the couch .\\n <endseq>', '<startseq> a little girl is bouncing on a couch in front of a large bay window .\\n <endseq>', '<startseq> a little girl jumping on a couch .\\n <endseq>', '<startseq> a small girl is jumping on a sofa bed .\\n <endseq>', '<startseq> a young girl jumps off of a couch and high into the air .\\n <endseq>']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD-tHX-vDicJ"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}