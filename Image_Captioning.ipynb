{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Image Captioning Basic Model CNN + LSTM\n"
      ],
      "metadata": {
        "id": "rdYCtezo__B5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Flickr8k DataSet for Image Captioning"
      ],
      "metadata": {
        "id": "OvGRvCzr29Ds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "ppiT9OXYH4NS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3ScHhExH--L",
        "outputId": "cbd87f4c-a196-4826-d7e1-fe7cbfd206c9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "archive = zipfile.ZipFile('drive/MyDrive/archive.zip')\n",
        "\n",
        "for file in archive.namelist():\n",
        "    archive.extract(file, 'CaptionData')"
      ],
      "metadata": {
        "id": "60GnSJQkd-ol"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Iwpk8IEIefxe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "caption_df = pd.read_csv('CaptionData/captions.txt')\n",
        "caption_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "1gUbsKNaelZW",
        "outputId": "d74b7d72-e6aa-48f1-999c-0373b3d4e70e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       image  \\\n",
              "0  1000268201_693b08cb0e.jpg   \n",
              "1  1000268201_693b08cb0e.jpg   \n",
              "2  1000268201_693b08cb0e.jpg   \n",
              "3  1000268201_693b08cb0e.jpg   \n",
              "4  1000268201_693b08cb0e.jpg   \n",
              "\n",
              "                                             caption  \n",
              "0  A child in a pink dress is climbing up a set o...  \n",
              "1              A girl going into a wooden building .  \n",
              "2   A little girl climbing into a wooden playhouse .  \n",
              "3  A little girl climbing the stairs to her playh...  \n",
              "4  A little girl in a pink dress going into a woo...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7b6d8a8a-12ca-4c49-b0ae-73822aeea0b1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>caption</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>A child in a pink dress is climbing up a set o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>A girl going into a wooden building .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>A little girl climbing into a wooden playhouse .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>A little girl climbing the stairs to her playh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>A little girl in a pink dress going into a woo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7b6d8a8a-12ca-4c49-b0ae-73822aeea0b1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7b6d8a8a-12ca-4c49-b0ae-73822aeea0b1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7b6d8a8a-12ca-4c49-b0ae-73822aeea0b1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-69ea845e-19aa-4e60-82c1-1e323720732e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-69ea845e-19aa-4e60-82c1-1e323720732e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-69ea845e-19aa-4e60-82c1-1e323720732e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "caption_df",
              "summary": "{\n  \"name\": \"caption_df\",\n  \"rows\": 40455,\n  \"fields\": [\n    {\n      \"column\": \"image\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8091,\n        \"samples\": [\n          \"3139895886_5a6d495b13.jpg\",\n          \"3133825703_359a0c414d.jpg\",\n          \"244910177_7c4ec3f65b.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"caption\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 40201,\n        \"samples\": [\n          \"A girl plays T-ball .\",\n          \"A woman in riding attire rides a jumping horse .\",\n          \"A brown dog wearing a pink shirt is followed by a brown dog wearing a yellow shirt .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_ids = list(caption_df['image'].values)\n",
        "len(image_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EDiQI9ogRuW",
        "outputId": "048629ad-3678-474f-98d6-10e64118aed0"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40455"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_image_ids = caption_df['image'].unique()"
      ],
      "metadata": {
        "id": "0kkWzvrggqlp"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess the Data"
      ],
      "metadata": {
        "id": "A2nTTDzN20wO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Images\n",
        "import os\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "IMAGE_SHAPE = (299, 299, 3)\n",
        "\n",
        "def preprocess_image(img_path):\n",
        "  img = image.load_img(img_path)\n",
        "  img = image.img_to_array(img)\n",
        "  img = img/255.0\n",
        "  img.resize(IMAGE_SHAPE)\n",
        "  img = preprocess_input(img)\n",
        "  img = np.expand_dims(img, axis=0)\n",
        "  return img\n"
      ],
      "metadata": {
        "id": "w4vZ9NGWeyvT"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = preprocess_image('/content/CaptionData/Images/1007129816_e794419615.jpg')\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V245KwNdt6uK",
        "outputId": "c996f52c-62d7-4a9f-f357-3358d18dcd06"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 299, 299, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For ease of computation here dealing with less number of images\n",
        "image_ids = image_ids[:5000]"
      ],
      "metadata": {
        "id": "q4_0PpM9_SYK"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def encode_images(image_dir, image_ids):\n",
        "  model = InceptionV3(weights='imagenet')\n",
        "  model = Model(model.input, model.layers[-2].output, name='feature_extractor')\n",
        "  image_features = {}\n",
        "\n",
        "  for image_id in image_ids:\n",
        "    if image_id not in image_features:\n",
        "      try:\n",
        "        img = preprocess_image(os.path.join(image_dir, image_id))\n",
        "        feature = model.predict(img, verbose=0)\n",
        "        image_features[image_id] = feature\n",
        "      except Exception as e:\n",
        "        print(f\"Error processing image {image_id}: {e}\")\n",
        "  return image_features\n",
        "\n",
        "image_dir = '/content/CaptionData/Images'\n",
        "image_features = encode_images(image_dir, image_ids)\n",
        "\n",
        "len(image_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4bCJXJJg5dr",
        "outputId": "9f9159c6-7642-47e6-e9a6-97111d8e0669"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Total number of unique images extracted: {len(image_features)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_6L7LIQt_oZ",
        "outputId": "c862a5cf-bd45-46fd-8035-29227bba6d86"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of unique images extracted: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('image_features.pkl', 'wb') as f:\n",
        "    pickle.dump(image_features, f)"
      ],
      "metadata": {
        "id": "eelWZ9JF5MH5"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Load the Captions\n",
        "\n",
        "captions_map = dict()\n",
        "all_captions = [] # populating to find out the vocabulary size\n",
        "\n",
        "\n",
        "def load_captions(captions_file_path):\n",
        "  with open(captions_file_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "    for line in lines:\n",
        "      tokens = line.split(',')\n",
        "      image_id, caption = tokens[0], ' '.join(tokens[1:]).lower()\n",
        "      if image_id in image_ids:\n",
        "        caption = '<startseq> ' + caption + ' <endseq>'\n",
        "        if image_id not in captions_map:\n",
        "          captions_map[image_id] = []\n",
        "\n",
        "        captions_map[image_id].append(caption)\n",
        "        all_captions.append(caption)\n",
        "\n",
        "load_captions('/content/CaptionData/captions.txt')"
      ],
      "metadata": {
        "id": "FFpa4UfAZXCo"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(captions_map)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcjLiPQtuXIk",
        "outputId": "8da06429-a3e6-4e7d-a124-d5adb61daa9c"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('captions_map.pkl', 'wb') as f:\n",
        "    pickle.dump(captions_map, f)"
      ],
      "metadata": {
        "id": "TLQvbc_8-hmR"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "m4dbFScf6_dV"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_captions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Vocabulary size:\", vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLI_RmhSyQTy",
        "outputId": "281d14db-a932-41c9-fbf6-63ce29df2890"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 3224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert captions to sequences\n",
        "sequences = tokenizer.texts_to_sequences(all_captions)\n",
        "\n",
        "# Find Max Length of a sequence\n",
        "max_length = max(len(seq) for seq in sequences)\n",
        "print(\"Max length of sequences:\", max_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWrvTzF1gjnw",
        "outputId": "efe24387-4cb4-45ad-e6eb-6780dcb6dff0"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max length of sequences: 35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import groupby\n",
        "from collections import Counter\n",
        "\n",
        "sequence_lengths = [len(seq) for seq in sequences]\n",
        "sequence_length_counts = dict(Counter(sequence_lengths))\n",
        "sequence_length_counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDVrqHA380Fr",
        "outputId": "020ec822-06fa-47c7-846f-08d7b41fc25c"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{19: 132,\n",
              " 9: 393,\n",
              " 10: 439,\n",
              " 11: 544,\n",
              " 14: 467,\n",
              " 17: 212,\n",
              " 20: 120,\n",
              " 21: 58,\n",
              " 22: 39,\n",
              " 15: 402,\n",
              " 16: 307,\n",
              " 13: 544,\n",
              " 18: 178,\n",
              " 12: 567,\n",
              " 7: 126,\n",
              " 8: 315,\n",
              " 25: 13,\n",
              " 4: 1,\n",
              " 24: 18,\n",
              " 26: 13,\n",
              " 6: 43,\n",
              " 28: 6,\n",
              " 23: 25,\n",
              " 29: 5,\n",
              " 31: 1,\n",
              " 27: 4,\n",
              " 5: 21,\n",
              " 35: 3,\n",
              " 30: 2,\n",
              " 32: 1,\n",
              " 33: 1}"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's say we take max caption token length is 20\n",
        "max_length = 20\n"
      ],
      "metadata": {
        "id": "cTYEKv_o-Mi2"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Training Data\n",
        "\n",
        "Training Data Input:\n",
        "\n",
        "\n",
        "\n",
        "*   Image\n",
        "*   Partial Caption output text so far\n",
        "\n",
        "\n",
        "Training Data Output:\n",
        "\n",
        "\n",
        "*   Next Caption token\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8O9wXf-g8817"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_training_data(captions_map, image_ids, image_features, tokenizer, max_length):\n",
        "  X1, X2, y = [], [], []\n",
        "  for image_id in image_ids:\n",
        "    captions = captions_map[image_id]\n",
        "    for caption in captions:\n",
        "      seq = tokenizer.texts_to_sequences([caption])[0]\n",
        "      for i in range(1, len(seq)):\n",
        "        in_seq, out_seq = seq[:i], seq[i]\n",
        "        in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "        # X1.append(np.squeeze(image_features[image_id], axis=0))\n",
        "        X1.append(image_features[image_id])\n",
        "        X2.append(in_seq)\n",
        "        y.append(out_seq)\n",
        "  return np.array(X1), np.array(X2), np.array(y)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H9XgojPk-qX6"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_ids = list(image_features.keys())"
      ],
      "metadata": {
        "id": "cDumg-Gu_aio"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_image_ids = image_ids[:800]\n",
        "val_image_ids = image_ids[800:900]\n",
        "test_image_ids = image_ids[900:]"
      ],
      "metadata": {
        "id": "5DonKOcjcBrS"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6tgSUAbQcQF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_X1, train_data_X2, train_data_y = generate_training_data(captions_map, train_image_ids, image_features, tokenizer, max_length)"
      ],
      "metadata": {
        "id": "giH1yTTpZAl7"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_X1.shape, train_data_X2.shape, train_data_y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46NxEFki_259",
        "outputId": "666467ad-ffc9-424a-9534-4117d5c20e00"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((48082, 1, 2048), (48082, 20), (48082, 3224))"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_data_X1, val_data_X2, val_data_y = generate_training_data(captions_map, val_image_ids, image_features, tokenizer, max_length)"
      ],
      "metadata": {
        "id": "ptX30SuKf8Ip"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the prepared Data\n",
        "np.save('train_data_X1.npy', train_data_X1)\n",
        "np.save('train_data_X2.npy', train_data_X2)\n",
        "np.save('train_data_y.npy', train_data_y)\n",
        "np.save('val_data_X1.npy', val_data_X1)\n",
        "np.save('val_data_X2.npy', val_data_X2)\n",
        "np.save('val_data_y.npy', val_data_y)\n",
        "np.save('test_data_X1.npy', test_data_X1)\n",
        "np.save('test_data_X2.npy', test_data_X2)\n",
        "np.save('test_data_y.npy', test_data_y)"
      ],
      "metadata": {
        "id": "X43z7nljqmze"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3o0CWTRaBZwl",
        "outputId": "ddf02326-fec7-4c4a-eefb-f462b5952b33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_29 (InputLayer)       [(None, 1, 2048)]            0         []                            \n",
            "                                                                                                  \n",
            " input_30 (InputLayer)       [(None, 20)]                 0         []                            \n",
            "                                                                                                  \n",
            " dense_13 (Dense)            (None, 1, 256)               524544    ['input_29[0][0]']            \n",
            "                                                                                                  \n",
            " embedding_4 (Embedding)     (None, 20, 256)              825344    ['input_30[0][0]']            \n",
            "                                                                                                  \n",
            " flatten (Flatten)           (None, 256)                  0         ['dense_13[0][0]']            \n",
            "                                                                                                  \n",
            " lstm_4 (LSTM)               (None, 256)                  525312    ['embedding_4[0][0]']         \n",
            "                                                                                                  \n",
            " add_4 (Add)                 (None, 256)                  0         ['flatten[0][0]',             \n",
            "                                                                     'lstm_4[0][0]']              \n",
            "                                                                                                  \n",
            " dense_14 (Dense)            (None, 256)                  65792     ['add_4[0][0]']               \n",
            "                                                                                                  \n",
            " dense_15 (Dense)            (None, 3224)                 828568    ['dense_14[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2769560 (10.57 MB)\n",
            "Trainable params: 2769560 (10.57 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, add, Flatten\n",
        "\n",
        "# Feature extractor (encoder) model\n",
        "inputs1 = Input(shape=(1, 2048,))\n",
        "fe1 = Dense(256, activation='relu')(inputs1)\n",
        "fe1 = Flatten()(fe1)\n",
        "\n",
        "# Sequence processor (decoder) model\n",
        "inputs2 = Input(shape=(max_length,))\n",
        "se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "se2 = LSTM(256)(se1)\n",
        "\n",
        "# Decoder (feed-forward) model\n",
        "decoder1 = add([fe1, se2])\n",
        "decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "# Combined model\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)"
      ],
      "metadata": {
        "id": "-m4eaR1jKDhw"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41Pi5UrPBsjf",
        "outputId": "4e7d2bcb-9ca1-4bba-d34a-2a82d8005ba1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1503/1503 - 61s - loss: 1.4959 - accuracy: 0.6073 - val_loss: 7.4749 - val_accuracy: 0.2812 - 61s/epoch - 40ms/step\n",
            "Epoch 2/20\n",
            "1503/1503 - 61s - loss: 1.3646 - accuracy: 0.6398 - val_loss: 8.0107 - val_accuracy: 0.2804 - 61s/epoch - 41ms/step\n",
            "Epoch 3/20\n",
            "1503/1503 - 60s - loss: 1.2514 - accuracy: 0.6710 - val_loss: 8.3940 - val_accuracy: 0.2844 - 60s/epoch - 40ms/step\n",
            "Epoch 4/20\n",
            "1503/1503 - 60s - loss: 1.1670 - accuracy: 0.6925 - val_loss: 8.7380 - val_accuracy: 0.2752 - 60s/epoch - 40ms/step\n",
            "Epoch 5/20\n",
            "1503/1503 - 60s - loss: 1.0997 - accuracy: 0.7105 - val_loss: 9.1460 - val_accuracy: 0.2774 - 60s/epoch - 40ms/step\n",
            "Epoch 6/20\n",
            "1503/1503 - 60s - loss: 1.0448 - accuracy: 0.7264 - val_loss: 9.6006 - val_accuracy: 0.2770 - 60s/epoch - 40ms/step\n",
            "Epoch 7/20\n",
            "1503/1503 - 61s - loss: 1.0000 - accuracy: 0.7388 - val_loss: 9.7736 - val_accuracy: 0.2715 - 61s/epoch - 40ms/step\n",
            "Epoch 8/20\n",
            "1503/1503 - 60s - loss: 0.9776 - accuracy: 0.7439 - val_loss: 10.0158 - val_accuracy: 0.2836 - 60s/epoch - 40ms/step\n",
            "Epoch 9/20\n",
            "1503/1503 - 60s - loss: 0.9457 - accuracy: 0.7511 - val_loss: 10.0384 - val_accuracy: 0.2742 - 60s/epoch - 40ms/step\n",
            "Epoch 10/20\n",
            "1503/1503 - 60s - loss: 0.9272 - accuracy: 0.7560 - val_loss: 10.5055 - val_accuracy: 0.2679 - 60s/epoch - 40ms/step\n",
            "Epoch 11/20\n",
            "1503/1503 - 60s - loss: 0.9107 - accuracy: 0.7590 - val_loss: 10.3366 - val_accuracy: 0.2693 - 60s/epoch - 40ms/step\n",
            "Epoch 12/20\n",
            "1503/1503 - 60s - loss: 0.9004 - accuracy: 0.7613 - val_loss: 10.7069 - val_accuracy: 0.2681 - 60s/epoch - 40ms/step\n",
            "Epoch 13/20\n",
            "1503/1503 - 60s - loss: 0.8833 - accuracy: 0.7666 - val_loss: 11.0079 - val_accuracy: 0.2677 - 60s/epoch - 40ms/step\n",
            "Epoch 14/20\n",
            "1503/1503 - 61s - loss: 0.8823 - accuracy: 0.7666 - val_loss: 10.8718 - val_accuracy: 0.2689 - 61s/epoch - 40ms/step\n",
            "Epoch 15/20\n",
            "1503/1503 - 60s - loss: 0.8690 - accuracy: 0.7691 - val_loss: 11.0804 - val_accuracy: 0.2652 - 60s/epoch - 40ms/step\n",
            "Epoch 16/20\n",
            "1503/1503 - 61s - loss: 0.8653 - accuracy: 0.7705 - val_loss: 11.1433 - val_accuracy: 0.2671 - 61s/epoch - 40ms/step\n",
            "Epoch 17/20\n",
            "1503/1503 - 61s - loss: 0.8586 - accuracy: 0.7705 - val_loss: 11.3138 - val_accuracy: 0.2693 - 61s/epoch - 40ms/step\n",
            "Epoch 18/20\n",
            "1503/1503 - 60s - loss: 0.8621 - accuracy: 0.7688 - val_loss: 11.4790 - val_accuracy: 0.2713 - 60s/epoch - 40ms/step\n",
            "Epoch 19/20\n",
            "1503/1503 - 60s - loss: 0.8487 - accuracy: 0.7723 - val_loss: 11.4868 - val_accuracy: 0.2696 - 60s/epoch - 40ms/step\n",
            "Epoch 20/20\n",
            "1503/1503 - 60s - loss: 0.8491 - accuracy: 0.7720 - val_loss: 11.4710 - val_accuracy: 0.2693 - 60s/epoch - 40ms/step\n"
          ]
        }
      ],
      "source": [
        "history = model.fit([train_data_X1, train_data_X2], train_data_y,\n",
        "          validation_data=([val_data_X1, val_data_X2], val_data_y),\n",
        "          epochs=20,\n",
        "          verbose=2,\n",
        "          callbacks=[callback]\n",
        "          )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('image_captioning_model_1.h5')"
      ],
      "metadata": {
        "id": "csFCw0UEM6ZF"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "1jYM42KxiWOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_image(image_path):\n",
        "  img = preprocess_image(image_path)\n",
        "  inception_v3 = InceptionV3(weights='imagenet')\n",
        "  inception_v3 = Model(inception_v3.input, inception_v3.layers[-2].output, name='feature_extractor')\n",
        "  feature = inception_v3.predict(img, verbose=0)\n",
        "  return feature"
      ],
      "metadata": {
        "id": "vtoHvor6C3e6"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4uaSKeFDmZR",
        "outputId": "49ee472d-d4e5-4bfc-bf93-1383501974d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Caption: <startseq> a man in a black shirt and jeans is kissing another man while he is holding a hello colored drink\n"
          ]
        }
      ],
      "source": [
        "def generate_caption(model, tokenizer, image, max_length):\n",
        "    in_text = '<startseq>'\n",
        "    for i in range(max_length):\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        # print(sequence.shape, image.shape)\n",
        "        yhat = model.predict([image, sequence], verbose=0)\n",
        "        yhat = np.argmax(yhat)\n",
        "        word = tokenizer.index_word[yhat]\n",
        "        if word is None:\n",
        "            break\n",
        "        in_text += ' ' + word\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    return in_text\n",
        "\n",
        "# Example usage\n",
        "img_path = 'fish-8896355_1280.jpg'\n",
        "image_feature = encode_image(img_path)\n",
        "image_feature = np.expand_dims(image_feature, axis=0)\n",
        "caption = generate_caption(model, tokenizer, image_feature, max_length)\n",
        "print(\"Generated Caption:\", caption)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Images\n",
        "\n",
        "test_image_features = [image_features[image_id] for image_id in test_image_ids]\n",
        "test_image_features = np.array(test_image_features)\n",
        "test_image_features.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbQEjEQVFJwh",
        "outputId": "ba9efd0d-ff0f-4a04-89a0-83311e0ad293"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 1, 2048)"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_captions = [captions_map[image_id] for image_id in test_image_ids]\n",
        "len(test_captions), test_captions[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESRrQ28SGJlf",
        "outputId": "55d05e62-3afe-4768-ef26-71a4a241d57e"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,\n",
              " ['<startseq> a group of horses and people in front of a snowy mountain .\\n <endseq>',\n",
              "  '<startseq> the riders and horses are taking a break and resting on the mountain trail .\\n <endseq>',\n",
              "  '<startseq> three men are standing around pack horses in front of a red tent up in the mountains .\\n <endseq>',\n",
              "  '<startseq> three riders stand around their horses in the mountains .\\n <endseq>',\n",
              "  '<startseq> two men and some horses on a snowy mountain .\\n <endseq>'])"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_caption(model, tokenizer, test_image_features[5], max_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "HvjgYpZiGcfJ",
        "outputId": "37a8ff8f-6635-4517-b49c-43512a024115"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<startseq> a man in a black shirt and jeans is kissing another man while he is holding a hello colored drink'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  print(generate_caption(model, tokenizer, test_image_features[i], max_length))\n",
        "  print(test_captions[i])\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2XRhK9vHycw",
        "outputId": "302fb40f-d8c8-4b58-db62-94f741e58f6e"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<startseq> a man in a black shirt and jeans is kissing another man while he is holding a hello colored drink\n",
            "['<startseq> a group of horses and people in front of a snowy mountain .\\n <endseq>', '<startseq> the riders and horses are taking a break and resting on the mountain trail .\\n <endseq>', '<startseq> three men are standing around pack horses in front of a red tent up in the mountains .\\n <endseq>', '<startseq> three riders stand around their horses in the mountains .\\n <endseq>', '<startseq> two men and some horses on a snowy mountain .\\n <endseq>']\n",
            "\n",
            "<startseq> a man in a black shirt and jeans is kissing another man while he is holding a hello colored drink\n",
            "['<startseq> a black and brown dog is biting on a stick in the forest .\\n <endseq>', '<startseq> a brown dog chewing on a large piece of wood .\\n <endseq>', '<startseq> a brown dog is chewing on a stick .\\n <endseq>', '<startseq> a dark brown dog is chewing on a stick .\\n <endseq>', '<startseq> the brown dog is playing with a stick .\\n <endseq>']\n",
            "\n",
            "<startseq> a man in a black shirt and jeans is kissing another man while he is holding a hello colored drink\n",
            "['<startseq> a black dog carrying a colorful ball swims .\\n <endseq>', '<startseq> a black dog is retrieving a ball in water .\\n <endseq>', '<startseq> a black dog is swimming with a ball in his mouth .\\n <endseq>', '<startseq> a black dog swims in water with a colorful ball in his mouth .\\n <endseq>', '<startseq> black dog paddles through the water with a bright ball in its mouth .\\n <endseq>']\n",
            "\n",
            "<startseq> a man in a black shirt and jeans is kissing another man while he is holding a hello colored drink\n",
            "['<startseq> a brown dog is jumping up at another dog in front of the man in jeans .\\n <endseq>', '<startseq> a tan dog is grappling another tan dog on gravel while someone stands narby .\\n <endseq>', '<startseq> two dogs engaged in physical contact with a man in the background .\\n <endseq>', '<startseq> two dogs play fighting on sand .\\n <endseq>', '<startseq> two dogs wrestle or hug .\\n <endseq>']\n",
            "\n",
            "<startseq> a man in a black shirt and jeans is kissing another man while he is holding a hello colored drink\n",
            "['<startseq> a hiker posing for a photo in an arid mountain landscape .\\n <endseq>', '<startseq> a man in a green hat poses in the mountains\\n <endseq>', '<startseq> a man in a green hat standing on a slope in front of a mountain .\\n <endseq>', '<startseq> \"a man in camouflage hiking gear   wearing a green pouch is standing in the foothills .\"\\n <endseq>', '<startseq> a person in hiking gear is standing on a hill with a mountain in the background .\\n <endseq>']\n",
            "\n",
            "<startseq> a man in a black shirt and jeans is kissing another man while he is holding a hello colored drink\n",
            "['<startseq> a boy gets flipped into the river by another boy .\\n <endseq>', '<startseq> a group of children play in the water under a bridge .\\n <endseq>', '<startseq> an airborne kid is watched by his launcher and others .\\n <endseq>', '<startseq> boys jump off a bridge into the water .\\n <endseq>', '<startseq> man in white shirt flipping young boy in the water with four other boys surrounding them .\\n <endseq>']\n",
            "\n",
            "<startseq> a man and woman sitting on a parked car endseq\n",
            "[\"<startseq> a little girl in a blue and pink leotard is walking along a beam whilst a person 's hand is stretched out to support her .\\n <endseq>\", '<startseq> a little girl in a blue and pink outfit is walking on a balance beam with the aid of woman standing to her left .\\n <endseq>', '<startseq> a little girl walking down a balance beam with an adult ready to assist .\\n <endseq>', '<startseq> a young girl is standing on a balance beam .\\n <endseq>', '<startseq> a young girl practicing gymnastics on a balance beam .\\n <endseq>']\n",
            "\n",
            "<startseq> a man in a black shirt and jeans is kissing another man while he is holding a hello colored drink\n",
            "['<startseq> a barefoot young boy glances at the camera while swinging in a park .\\n <endseq>', '<startseq> a boy swings high under blue sky .\\n <endseq>', '<startseq> a boy wearing brown swings in a playground against a blue sky .\\n <endseq>', '<startseq> a young sandy blonde boy is at a park swinging on the swings .\\n <endseq>', '<startseq> two children at the height of a swing .\\n <endseq>']\n",
            "\n",
            "<startseq> a man in a black shirt and jeans is kissing another man while he is holding a hello colored drink\n",
            "['<startseq> the men are climbing .\\n <endseq>', '<startseq> two men are lying in red cots on the side of a mountain .\\n <endseq>', '<startseq> two men are sleeping on makeshift beds on the side of a cliff .\\n <endseq>', '<startseq> two men in cots hanging from a cliff\\n <endseq>', '<startseq> two mountain climbers are smiling on the mountainside .\\n <endseq>']\n",
            "\n",
            "<startseq> a man in a black shirt and jeans is kissing another man while he is holding a hello colored drink\n",
            "['<startseq> a little girl in pajamas is jumping on the couch .\\n <endseq>', '<startseq> a little girl is bouncing on a couch in front of a large bay window .\\n <endseq>', '<startseq> a little girl jumping on a couch .\\n <endseq>', '<startseq> a small girl is jumping on a sofa bed .\\n <endseq>', '<startseq> a young girl jumps off of a couch and high into the air .\\n <endseq>']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD-tHX-vDicJ"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}